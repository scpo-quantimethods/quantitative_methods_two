[
  {
    "objectID": "sessions/session3.html",
    "href": "sessions/session3.html",
    "title": "Regression discontinuity",
    "section": "",
    "text": "About this site\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# simulate the data\ndat &lt;- tibble(\n    x = rnorm(1000, 50, 25)\n) %&gt;%\n    mutate(\n        x = if_else(x &lt; 0, 0, x)\n    ) %&gt;%\n    filter(x &lt; 100)\n\n# cutoff at x = 50\ndat &lt;- dat %&gt;% \n    mutate(\n        D  = if_else(x &gt; 50, 1, 0),\n        y1 = 25 + 0 * D + 1.5 * x + rnorm(n(), 0, 20)\n    )\n\nggplot(aes(x, y1, colour = factor(D)), data = dat) +\n    geom_point(alpha = 0.5) +\n    geom_vline(xintercept = 50, colour = \"grey\", linetype = 2)+\n    stat_smooth(method = \"lm\", se = F) +\n    labs(x = \"Test score (X)\", y = \"Potential Outcome (Y1)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ndat &lt;- dat %&gt;%\n  mutate(\n    y2 = 25 + 40 * D + 1.5 * x + rnorm(n(), 0, 20)\n  )\n\n# figure\nggplot(aes(x, y2, colour = factor(D)), data = dat) +\n  geom_point(alpha = 0.5) +\n  geom_vline(xintercept = 50, colour = \"grey\", linetype = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  labs(x = \"Test score (X)\", y = \"Potential Outcome (Y)\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "sessions/session1.html",
    "href": "sessions/session1.html",
    "title": "Session 1 — Descriptive statistics",
    "section": "",
    "text": "Introduction\n\n\nTheoretical background\n\n\nExercices\n\n1 + 1\n\n[1] 2\n\n\n\\[\n\\begin{equation}\n\\hat{\\beta} = (X'X)^{-1}X'y\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n\\hat{\\beta} = (A'A)^{-1}A'y\n\\end{equation}\n\\]\nSecond example:\n\nRStataPython\n\n\n\n1 + 2\n\n[1] 3\n\n\n\n\n1 + 2\n\n\n1 + 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "sessions/session1/session1.html",
    "href": "sessions/session1/session1.html",
    "title": "Session 1 — R reminder and OLS",
    "section": "",
    "text": "The canonical form of a research enigma, or research puzzle, lays in this simple relationship : a dependent variable X, remains puzzling despite knowing a series of relationships, X that affect this variable. The researcher’s task generally comes with identifying an other variable, Z - or series of variables -, which might explain and cause X, despite knowing Y. That is generally how a research project can be formulated : why Y despite X ? Because Z.\nThe typical task of a research problem is therefore to link dependent variables with explanatory variables, Z, and to distinguish them from other types of variables, X, which do typically originate from previous research or the researcher’s intuition, and which might also affect the variable Y. Of course, isolating such causation relationships is anything but trivial, and the whole point of this semester’s class will be to convince you that correlation and causation have nothing in common.\nThe most used model in quantitative social science to establish such causation relationships is called the linear regression model: it is the workhorse of quantitative analysis in social science. This model generally assumes that X and Y evolve together in a linear way - that is, any increase in X, noted \\(\\Delta X\\), has an impact \\(\\beta \\Delta X\\) on \\(Y\\), where \\(\\beta\\) is constant. The objective of this session is therefore to :\n\nremind you some basics in R\nremind you some basics in descriptive statistics\nremind you what simple regressions and multiple regressions\n\nAll these notions will be crucial for the rest of the semester."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative methods II",
    "section": "",
    "text": "This website supports the teaching of the class of quantitative methods II, taught in the School of Research of Sciences Po. The tutorials are supporting the theoretical class given by Jan Rovny and Brenda Van Coppennolle, and aims to remind students notions about R, OLS regressions, as well as to introduce them to causal inference methods used in social science, all based on practical coding exercices on R. This class is built on numerous sources that have contributed to inspire our theory and exercices.\nFrom current or former instructors of SciencesPo, an introduction to R coding is available on Malo Jan’s website (RStudio introduction course). Different coding examples are also provided by Jan Rovny, on his website. This class has also been taught last year by Luis Sattelmayer, whose site provides detailed information, exercices appendices, from which this website is often directly or indirectly inspired. Moreover, there exists a wealth of ressources online, which have also guided the making of this class. To cite only but a few of them :\nThough this class is considered “advanced”, it is also thought of and conceived as a way to remind important notions in statistics, as well as basics of coding, already seen in the class of quantitative methods I. So rest reassured that, as much as we can, we will (1) remind previous notions of coding and statistics, (2) advance in a rhythm that is suitable for everyone.\nThe teaching assistants for this class are :\n\nJessica de Rongé, who is reachable at jessica.deronge@sciencespo.fr\nPaul Servais, who can be contacted at paul.servais@sciencespo.fr"
  },
  {
    "objectID": "index.html#homeworks",
    "href": "index.html#homeworks",
    "title": "Quantitative methods II",
    "section": "Homeworks",
    "text": "Homeworks\nIn order to assess your progress throughout the semester, two homeworks - each worth 15% of the final grade -, will be organised at the end of week 2, and at the end of week 4. You will receive the guidelines for these homeworks at that moment, and you will have two weeks (hence, until the next session) to upload your solutions to these problems on Moodle. These homeworks will be conceived to only check your advancement in the class, and not to overload you with burdensome work. Our goal is absolutely not to overload you with work, but mainly to control for your engagement with the course material. Don’t be worried if you don’t manage to fully complete the exercices or if your code is imperfect, we understand and know that getting into coding can be hard and that there is a quite high entry cost to this kind of course content - our main ambition for you is that you try and get your hands in the mud - otherwise coding is impossible to learn."
  },
  {
    "objectID": "index.html#final-paper",
    "href": "index.html#final-paper",
    "title": "Quantitative methods II",
    "section": "Final paper",
    "text": "Final paper\nAt the end of the class, you will be asked to write a quantitative methods paper, on a topic of your choice, using data of your choice.\n#Requirements You will need RStudio for this class, and should have installed it and be ready to use it before the start of the class. If you do not know how to download it, please follow the instructions available there, there or there. Download a file and try to open it on RStudio, to make sure that your environment is working."
  },
  {
    "objectID": "sessions/session1/session1.html#packages",
    "href": "sessions/session1/session1.html#packages",
    "title": "Session 1 — R reminder and OLS",
    "section": "Packages",
    "text": "Packages\nLast semester, with Noémie, Charlotte or Jean-Baptiste, you’ve probably learnt to code with tidyverse, which helps a lot in data manipulation, and is called a package, and can be downloaded throughinstall.packages(). In this class, we will also use a series of packages that will make our life a lot easier, like. This is how you can load and then use these packages in your code :\n\n#install.packages(\"package_name\") if you don't have these packages yet in your environment\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rio) #helps, through import() and export() function, to not care too much about our data format\nlibrary(stargazer) #produces beautiful tables as outputs\n\n\nPlease cite as: \n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nlibrary(readxl) # allows you to read excel file"
  },
  {
    "objectID": "sessions/session1/session1.html#importing-data",
    "href": "sessions/session1/session1.html#importing-data",
    "title": "Session 1 — R reminder and OLS",
    "section": "Importing data",
    "text": "Importing data\nNext, you may want to basically import data in the environment. This can be done using either the absolute path of your data ~/Desktop/Code/data/file.xslx, or the relative path. Here, since I have stored my data in a data folder located in the same folder as my main file (on which I am writing these lines), I only have to specify this relative path data/file.csv. I am importing data on the European elections in “France Métropolitaine” (with departements from 1 to 95), provided by the Ministère de l’Intérieur and slightly recoded (right = LR, far_right = RN, incumbents = Macron, socialists = PS, Greens = Verts). I am also importing socio-demographic data mainly originating from Cagé & Piketty.\n\nelections2024 &lt;- read_csv(\"data/elections2024.csv\", show_col_types = FALSE)\nsocio_demo &lt;- read_excel(\"data/demographics2024.xlsx\")\n#you can also do with rio if you don't want to bother with the file format\nelections2024 &lt;- import(\"data/elections2024.csv\",show_col_types = FALSE)\nsocio_demo &lt;- import(\"data/demographics2024.xlsx\")\n#You can also download other data from the internet (here from the Chapell Hill Experts Survey):\ndataset_from_internet_1 &lt;- import(\"https://www.chesdata.eu/s/1999-2024_CHES_dataset_means.csv\")\n\nNext, you may want to inspect the data that you have downloaded :\n\nglimpse(socio_demo)\n\nRows: 34,596\nColumns: 43\n$ codecommune         &lt;chr&gt; \"01001\", \"01002\", \"01004\", \"01005\", \"01006\", \"0100…\n$ Year                &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 20…\n$ codedep             &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"0…\n$ agri                &lt;dbl&gt; 0, 0, 4, 11, 0, 15, 0, 0, 29, 11, 11, 2, 0, 11, 0,…\n$ indp                &lt;dbl&gt; 39, 39, 142, 37, 0, 39, 19, 41, 36, 0, 0, 0, 119, …\n$ cadr                &lt;dbl&gt; 33, 9, 678, 56, 0, 250, 18, 0, 58, 2, 0, 18, 19, 1…\n$ pint                &lt;dbl&gt; 14, 0, 1647, 192, 34, 311, 83, 95, 98, 78, 0, 13, …\n$ empl                &lt;dbl&gt; 86, 66, 1211, 185, 23, 339, 83, 21, 197, 17, 53, 1…\n$ ouvr                &lt;dbl&gt; 147, 0, 1235, 177, 23, 268, 122, 4, 154, 3, 68, 42…\n$ chom                &lt;dbl&gt; 26, 9, 427, 18, 0, 95, 7, 2, 47, 9, 9, 0, 22, 37, …\n$ pact                &lt;dbl&gt; 319, 114, 4917, 658, 80, 1222, 325, 161, 572, 111,…\n$ age                 &lt;dbl&gt; 44.20355, 36.97382, 39.07810, 43.10032, 44.01273, …\n$ popf                &lt;dbl&gt; 380, 128, 7490, 802, 56, 1613, 391, 191, 601, 173,…\n$ francais            &lt;dbl&gt; 776, 256, 12645, 1700, 107, 2867, 743, 316, 1111, …\n$ etranger            &lt;dbl&gt; 4, 1, 1248, 23, 5, 64, 18, 8, 37, 5, 5, 0, 402, 11…\n$ prixbien            &lt;dbl&gt; 231440.42, 136645.75, 190080.39, 228015.31, 172697…\n$ capitalratio        &lt;dbl&gt; 1.0423332, 0.6154085, 0.8560610, 1.0269077, 0.7777…\n$ npropri             &lt;dbl&gt; 345, 102, 3000, 464, 51, 845, 255, 149, 387, 126, …\n$ nlogement           &lt;dbl&gt; 418, 126, 7209, 604, 77, 1094, 354, 182, 524, 147,…\n$ pop                 &lt;dbl&gt; 747, 288, 14375, 1717, 116, 3057, 797, 325, 1300, …\n$ revmoyfoy           &lt;dbl&gt; 35802.43, 36150.97, 29982.14, 37242.09, 31769.92, …\n$ revtot              &lt;dbl&gt; 1.277961e-05, 3.862623e-06, 2.096039e-04, 2.853238…\n$ recetteimpotslocaux &lt;dbl&gt; 233.1938, 240.6562, 503.5779, 355.4615, 312.5804, …\n$ recette             &lt;dbl&gt; 608.9172, 796.6805, 1067.3215, 651.9904, 1026.0869…\n$ baseimpotslocaux    &lt;dbl&gt; 1905.169, 2244.090, 2895.855, 1900.238, 2077.438, …\n$ nodip               &lt;dbl&gt; 319, 57, 5361, 817, 0, 1026, 201, 69, 532, 124, 20…\n$ sup                 &lt;dbl&gt; 134, 15, 2884, 208, 1, 642, 201, 182, 197, 87, 9, …\n$ Libellé             &lt;chr&gt; \"L'Abergement-Clémenciat\", \"L'Abergement-de-Varey\"…\n$ Exploitations       &lt;dbl&gt; 11, 2, 2, 9, NA, 17, NA, 6, 10, 3, 5, 4, NA, 13, 4…\n$ Personnes           &lt;dbl&gt; 21, 6, 3, 17, NA, 30, NA, 13, 17, 6, 9, 15, NA, 19…\n$ UTA                 &lt;dbl&gt; 16, 6, 2, 11, NA, 25, NA, 9, 8, 7, 5, 16, NA, 8, 9…\n$ SAU                 &lt;dbl&gt; 889, 195, 71, 821, NA, 1888, NA, 122, 326, 584, 25…\n$ PBS                 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ dens                &lt;chr&gt; \"Rural\", \"Rural\", \"Urbain intermédiaire\", \"Rural\",…\n$ densaav             &lt;chr&gt; \"Rural non périurbain\", \"Rural non périurbain\", \"U…\n$ dens7               &lt;chr&gt; \"Rural à habitat dispersé\", \"Rural à habitat dispe…\n$ ntot                &lt;dbl&gt; 345, 123, 5344, 676, 80, 1317, 332, 163, 619, 120,…\n$ pagri               &lt;dbl&gt; 0.000000000, 0.000000000, 0.000748503, 0.016272189…\n$ pindp               &lt;dbl&gt; 0.11304348, 0.31707317, 0.02657186, 0.05473373, 0.…\n$ pcadr               &lt;dbl&gt; 0.09565217, 0.07317073, 0.12687126, 0.08284024, 0.…\n$ ppint               &lt;dbl&gt; 0.04057971, 0.00000000, 0.30819611, 0.28402367, 0.…\n$ pempl               &lt;dbl&gt; 0.24927536, 0.53658537, 0.22660928, 0.27366864, 0.…\n$ pouvr               &lt;dbl&gt; 0.42608696, 0.00000000, 0.23110030, 0.26183432, 0.…\n\nglimpse(elections2024)\n\nRows: 34,400\nColumns: 15\n$ codecommune         &lt;int&gt; 1001, 1002, 1004, 1005, 1006, 1007, 1008, 1009, 10…\n$ codedep             &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ election_type       &lt;chr&gt; \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", …\n$ Year                &lt;int&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 20…\n$ far_right_relvotes  &lt;dbl&gt; 0.4281843, 0.2000000, 0.3453984, 0.4756258, 0.3692…\n$ far_right_votes     &lt;int&gt; 158, 30, 1565, 361, 24, 493, 144, 59, 191, 85, 59,…\n$ turnout             &lt;dbl&gt; 0.5574018, 0.6578947, 0.5203261, 0.5689655, 0.6435…\n$ incumbents_relvotes &lt;dbl&gt; 0.14363144, 0.14666667, 0.10836460, 0.14624506, 0.…\n$ incumbents_votes    &lt;int&gt; 53, 22, 491, 111, 15, 140, 39, 23, 56, 11, 21, 10,…\n$ greens_relvotes     &lt;dbl&gt; 0.05691057, 0.13333333, 0.04789230, 0.03162055, 0.…\n$ greens_votes        &lt;int&gt; 21, 20, 217, 24, 3, 65, 14, 3, 18, 15, 3, 3, 18, 1…\n$ right_relvotes      &lt;dbl&gt; 0.06504065, 0.06000000, 0.06488634, 0.06060606, 0.…\n$ right_votes         &lt;int&gt; 24, 9, 294, 46, 5, 83, 20, 15, 9, 6, 10, 2, 68, 26…\n$ socialists_relvotes &lt;dbl&gt; 0.07317073, 0.12000000, 0.12425513, 0.09090909, 0.…\n$ socialists_votes    &lt;int&gt; 27, 18, 563, 69, 9, 156, 40, 28, 35, 18, 27, 5, 10…\n\n\nHere you see that some values, like PBS have many NA’s and you may want to get rid of some values that are unnecessary for your purposes. You can do that by using the select function\n\nsocio_demo &lt;- socio_demo %&gt;% select(-c(Personnes, Exploitations, UTA, SAU, PBS))\n\nImagine that you are only interested in the . You can inspect the values taken in a column with a categorical variable by looking at\n\nunique(socio_demo$dens7)\n\n[1] \"Rural à habitat dispersé\"       \"Centres urbains intermédiaires\"\n[3] \"Bourgs ruraux\"                  \"Ceintures urbaines\"            \n[5] \"Rural à habitat très dispersé\"  \"Petites villes\"                \n[7] \"Grands centres urbains\"        \n\ntable(socio_demo$dens7)\n\n\n                 Bourgs ruraux             Ceintures urbaines \n                          5039                           1963 \nCentres urbains intermédiaires         Grands centres urbains \n                           614                            690 \n                Petites villes       Rural à habitat dispersé \n                           935                          18273 \n Rural à habitat très dispersé \n                          7082 \n\n\nAnd select a subpart of this data set with filter:\n\nsocio_demo_rural &lt;- socio_demo %&gt;% filter(dens == \"Rural\") %&gt;% filter(dens7 == \"Rural à habitat dispersé\"|dens7 == \"Rural à habitat très dispersé\")\ntable(socio_demo_rural$dens7)\n\n\n     Rural à habitat dispersé Rural à habitat très dispersé \n                        18273                          7082 \n\n\nYou can then check the summary of that variable and its distribution :\n\nsummary(socio_demo_rural$pagri)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n0.00000 0.00000 0.00000 0.07257 0.09091 1.00000     144 \n\n\n\nHistogram 1Histogram 2Histogram 3\n\n\n\nhist(socio_demo_rural$pagri, breaks = 20)\n\n\n\n\n\n\n\n\n\n\n\nhist(socio_demo_rural$revmoyfoy, breaks = 20)\n\n\n\n\n\n\n\n\n\n\n\nhist(log(socio_demo_rural$revmoyfoy), breaks = 20)\n\n\n\n\n\n\n\n\n\n\n\nThe summary() command informs you that your data set contains 144 NA’s. You can decide to remove these values, or to fill those with the mean of the variable - so that in a later stage, these data are not changing the estimation.\n\nsocio_demo_rural &lt;- socio_demo_rural %&gt;%   \n  mutate(pagri = case_when(\n    is.na(pagri) ~ mean(pagri, na.rm = TRUE),\n    .default =pagri\n  ))\n\nsummary(socio_demo_rural$pagri)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.00000 0.00000 0.07257 0.09039 1.00000 \n\n\nWhere you see that the mean hasn’t changed, but the NA’s are all gone. Now imagine that you would like to analyse the parties’ vote share depending on the context : you would like to know if in ‘urban/rural’ communes, people vote more or less for the different parties.\nFor this you may need to combine electoral results with data on the communes themselves, and then join those. This is possible because your have a unique identifier per commune : commune 01001 is the same individual in socio_demo as 1001 in elections2024, it is therefore to merge these guys together. Note here that you first have to convert the commune codes to numerical values in order for the merge to operate, so that codecommune and codedep have the same type in both data set (as you can see with the earlier glimpses, this was not the case beforehand). Otherwise, the merging does not work because the computer does not understand what are the common variables to merge on.\n\nsocio_demo &lt;- socio_demo %&gt;% mutate(codecommune = as.numeric(codecommune), codedep = as.numeric(codedep)) %&gt;% filter(!is.na(codecommune), !is.na(codedep))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `codecommune = as.numeric(codecommune)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\nelections2024 &lt;- elections2024 %&gt;% left_join(socio_demo)\n\nJoining with `by = join_by(codecommune, codedep, Year)`\n\nlibrary(DT)\ndatatable(head(elections2024,300))"
  },
  {
    "objectID": "sessions/session1/session1.html#histogram-2",
    "href": "sessions/session1/session1.html#histogram-2",
    "title": "Session 1 — R reminder and OLS",
    "section": "",
    "text": "hist(socio_demo_rural$revmoyfoy, breaks = 20)"
  },
  {
    "objectID": "sessions/session1/session1.html#histogram-3",
    "href": "sessions/session1/session1.html#histogram-3",
    "title": "Session 1 — R reminder and OLS",
    "section": "",
    "text": "hist(log(socio_demo_rural$revmoyfoy), breaks = 20)\n\n\n\n\n\n\n\n\n:::\nThe summary() command informs you that your data set contains 144 NA’s. You can decide to remove these values, or to fill those with the mean of the variable - so that in a later stage, these data are not changing the estimation.\n\nsocio_demo_rural &lt;- socio_demo_rural %&gt;%   \n  mutate(pagri = case_when(\n    is.na(pagri) ~ mean(pagri, na.rm = TRUE),\n    .default =pagri\n  ))\n\nsummary(socio_demo_rural$pagri)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.00000 0.00000 0.00000 0.07257 0.09039 1.00000 \n\n\nWhere you see that the mean hasn’t changed, but the NA’s are all gone. Now imagine that you would like to analyse the parties’ vote share depending on the context : you would like to know if in ‘urban/rural’ communes, people vote more or less for the different parties.\nFor this you may need to combine electoral results with data on the communes themselves, and then join those. This is possible because your have a unique identifier per commune : commune 01001 is the same individual in socio_demo as 1001 in elections2024, it is therefore to merge these guys together. Note here that you first have to convert the commune codes to numerical values in order for the merge to operate, so that codecommune and codedep have the same type in both data set (as you can see with the earlier glimpses, this was not the case beforehand). Otherwise, the merging does not work because the computer does not understand what are the common variables to merge on.\n\nsocio_demo &lt;- socio_demo %&gt;% mutate(codecommune = as.numeric(codecommune), codedep = as.numeric(codedep)) %&gt;% filter(!is.na(codecommune), !is.na(codedep))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `codecommune = as.numeric(codecommune)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\nelections2024 &lt;- elections2024 %&gt;% left_join(socio_demo)\n\nJoining with `by = join_by(codecommune, codedep, Year)`\n\nlibrary(DT)\ndatatable(head(elections2024,300))"
  },
  {
    "objectID": "sessions/session1/session1.html#base-r",
    "href": "sessions/session1/session1.html#base-r",
    "title": "Session 1 — R reminder and OLS",
    "section": "Base R",
    "text": "Base R\nR is a language that comes with a basic infrastructure, which is called Base R, and is able to perform a wide array of basic tasks. This basic infrastructure is the basic tools that you get when you download R. In order to facilitate everyone’s life, very nice and smart people have built additional coding tools, which we only have to install, and that allow us to perform complex tasks in an extremely fast and elegant manner."
  },
  {
    "objectID": "sessions/session1/session1.html#errors-and-help",
    "href": "sessions/session1/session1.html#errors-and-help",
    "title": "Session 1 — R reminder and OLS",
    "section": "Errors and help()",
    "text": "Errors and help()\nIn general, many errors arrive because of syntax or problems in the imports of data. Malo Jan has provided a helpful guide over these different kinds of errors, which I encourage you to read, as this can definitely help you. Moreover, for any function, you can always call the function help(function_name), which will provide you with the R documentation on that function."
  },
  {
    "objectID": "sessions/session1/session1.html#interpretation",
    "href": "sessions/session1/session1.html#interpretation",
    "title": "Session 1 — R reminder and OLS",
    "section": "Interpretation",
    "text": "Interpretation"
  },
  {
    "objectID": "sessions/session1/session1.html#ecological-fallacy",
    "href": "sessions/session1/session1.html#ecological-fallacy",
    "title": "Session 1 — R reminder and OLS",
    "section": "Ecological fallacy",
    "text": "Ecological fallacy"
  },
  {
    "objectID": "sessions/session1/session1.html#interpretation-of-regression-results",
    "href": "sessions/session1/session1.html#interpretation-of-regression-results",
    "title": "Session 1 — R reminder and OLS",
    "section": "Interpretation of regression results",
    "text": "Interpretation of regression results\nHere, you see that we have many information. The first thing we want to identify is the structure of the table. In the left column, we have our independent variable—our predictor. On the right-hand side, we have the corresponding coefficients, some of which have stars. In parentheses and underneath each coefficient, we find the standard errors. Smaller standard errors suggest more precise estimates.\nThe stars tell us something about statistical significance. If a variable has a p-value below a certain threshold, it will be marked with one or more stars. A single star * denotes significance at the 5% level (p &lt; 0.05), two stars ** indicate significance at the 1% level (p &lt; 0.01), and three stars *** show significance at the 0.1% level (p &lt; 0.001). This means that a specific variable has a statistically significant relationship with the dependent variable and contributes to explaining its variance. However, the absence of statistical significance does not necessarily mean that the variable has no effect—it may suggest that there is insufficient evidence to detect a meaningful relationship given the data.\nThe coefficients tell us about the direction and strength of the association between a statistically significant predictor and our dependent variable. A positive coefficient indicates that an increase in the independent variable is associated with an increase in the dependent variable, while a negative coefficient suggests that an increase in the independent variable corresponds to a decrease in the dependent variable. Since this is an ordinary least squares (OLS) regression, we can interpret the coefficients directly. However, because the independent variable is logged, the interpretation must be made in proportional (percentage) terms rather than in raw units.\nLet’s interpret the above regression table. Always bear in mind what a change of values would correspond to on the measurement scale of your dependent variable. In our case, higher values of the dependent variable correspond to a higher share of votes for far-right parties.\nHousehold income (logged) is negatively and highly significantly associated with the share of far-right votes. The coefficient is −0.186 and is statistically significant at the 0.1% level. This means that, in intermediate urban areas, an increase in average household income is associated with a decrease in far-right electoral support. More precisely, a 1% increase in average household income is associated with approximately a 0.19-point decrease in the share of far-right votes. This suggests that wealthier municipalities tend to vote less for far-right parties.\nThe intercept represents the expected level of far-right vote share when the logged income variable equals zero. While this value (2.31) is not substantively meaningful on its own, it is necessary for correctly estimating the relationship between income and far-right voting.\nBear in mind that this model is a very simple one. It includes only one independent variable and is therefore likely to omit many important predictors of far-right voting, such as education levels, unemployment, age structure, immigration levels, or political history. As a result, the model may suffer from omitted variable bias, meaning that the estimated effect of income may be capturing the influence of other unobserved factors correlated with income. While the negative association is strong and statistically significant, it should not be interpreted as a causal effect.\nThere is also some additional information in the table that tells us more about the model’s fit and overall performance:\n\nR²: The R-squared value is 0.263, meaning that approximately 26.3% of the variation in far-right vote share across intermediate urban areas is explained by average household income alone. This is relatively high for a single-variable model, suggesting that income is an important correlate of far-right voting.\nAdjusted R²: The adjusted R-squared (0.2632) is almost identical to the R-squared value, which is expected given that the model includes only one predictor. This indicates that the explanatory power of the model is not artificially inflated by unnecessary variables.\nResidual Standard Error: The residual standard error is 0.084, which measures the average distance between the model’s predicted values and the observed far-right vote shares. Lower values indicate better predictive accuracy; here, the error suggests a moderate level of unexplained variation remains.\nF-statistic: The F-statistic is very large (1248) and highly statistically significant, indicating that the model performs far better than a model with no predictors at all. In other words, income significantly improves our ability to explain variation in far-right electoral support.\n\nOverall, this regression suggests a strong and statistically significant negative relationship between household income and far-right voting in intermediate urban areas, while also highlighting the limits of inference from a highly simplified model."
  },
  {
    "objectID": "sessions/session1/session1.html#interactions",
    "href": "sessions/session1/session1.html#interactions",
    "title": "Session 1 — R reminder and OLS",
    "section": "Interactions",
    "text": "Interactions\nYou may now wonder how heterogeneous your results are, depending on one another. Indeed, perhaps the effect of income on the far-right vote is conditional on another variable, that is for instance the level of education ; and vice-versa. In other words, the results might be heterogeneous : a rich commune with less educated people can potentially vote differently than a rich commune with many higher educated persons. This is what is meant by checking for interactions between independent variables. For instance, in the case of a regression model with three variables, instead of checking :\n\\[\\begin{equation}\nY_i = \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i},\n\\end{equation}\\] you try : \\[\\begin{equation}\nY_i = \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + \\beta_4 X_{1i}*X_{3i}\n\\end{equation}\\]\nWhich means that any increase \\(\\Delta X_{1i}\\) in the variable \\(X_{1i}\\) no longer has an impact \\(\\beta_1 \\Delta X_{1i}\\) on \\(Y_i\\), but an impact \\(\\Delta X_{1i} (\\beta_1 + \\beta_3X_{3i})\\), which therefore still depends on the variable \\(X_{3i}\\). Coming back to our example, we want to assess how income and education in a commune evolve and influence far-right voting. You only need to specify * in your regression to obtain the interaction :\n\nmodel3 &lt;- lm(data = elections2024 %&gt;% filter(dens == \"Urbain intermédiaire\") %&gt;% mutate(petr = etranger/pop, psup = sup/pop, pnodip = nodip/pop, log_revmoyfoy = log(revmoyfoy)) , far_right_relvotes ~ log_revmoyfoy +petr+ pagri+pouvr+ pcadr+age+log(prixbien)+pempl+ppint+pindp+psup+pnodip + log_revmoyfoy*psup)\n\nlibrary(ggeffects)\n\nggpredict(model3, terms = c(\"log_revmoyfoy\", \"psup\")) |&gt; \n  as_tibble() |&gt; \n  mutate(\n    group = factor(\n      group,\n      levels = c(\"0.14\", \"0.23\", \"0.32\"),\n      labels = c(\n        \"Low share of higher education (14%)\",\n        \"Medium share of higher education (23%)\",\n        \"High share of higher education (32%)\"\n      )\n    )\n  ) |&gt; \n  ggplot(aes(x, predicted, color = group, fill = group)) + \n  geom_line(linewidth = 1) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2) +\n  labs(\n    color = \"Share with higher education\",\n    fill  = \"Share with higher education\"\n  )\n\n\n\n\n\n\n\n\nAnd you see that your predicted values change with the level of education : in the more educated cases, the line becomes less steep. Here, only three values of education are plotted by the program to give an idea of the interaction in three scenarios (commune with low/medium/high proportion of educated people), but this shows that the more educated people there are in a commune, the less the impact of income on the RN vote will be important in that commune."
  },
  {
    "objectID": "sessions/session1/session1.html#the-ecological-fallacy",
    "href": "sessions/session1/session1.html#the-ecological-fallacy",
    "title": "Session 1 — R reminder and OLS",
    "section": "The ecological fallacy",
    "text": "The ecological fallacy\nIt is important to note that, since your observations are at the commune level, your conclusions can only be drawn at the commune level. As such, even if you observe a strong correlation between RN vote share and the presence of blue-collar workers at the commune level, this does not allow you to conclude that blue-collar workers are more likely to vote for the radical right. This would be an example of the ecological fallacy, which consists of deriving individual-level behavior from aggregate-level data.\nIn fact, the presence of blue-collar workers in these communes may be correlated with other factors that also affect the RN vote : like living on the country-side, where other categories of the population might vote RN - and therefore lead to a strong correlation between working class voting behaviour and RN voting. It is only thanks to other studies using individual-level data that we know that the RN is the preferred party among workers (after abstention)."
  },
  {
    "objectID": "sessions/session1/session1.html#multicollinearity",
    "href": "sessions/session1/session1.html#multicollinearity",
    "title": "Session 1 — R reminder and OLS",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nIt’s essential to evaluate the correlation between predictors in regression analysis, as highly correlated predictors can introduce multicollinearity. This occurs when predictors capture overlapping information, which can distort the reliability of the model’s estimates.\nA useful tool for diagnosing multicollinearity is the Variance Inflation Factor (VIF), which measures how much the variance of a regression coefficient increases due to correlation among predictors. A VIF value above 10 generally signals problematic multicollinearity. Another helpful metric is tolerance (calculated as 1/VIF), which indicates the proportion of a predictor’s variance not explained by other predictors. Tolerance values near 1 suggest low collinearity. If multicollinearity is detected, techniques like Principal Component Analysis (PCA) can help address the issue.\nIn R, you can assess multicollinearity using the check_collinearity() function from the performance package (or the car::vif function). These functions compute VIF, provide confidence intervals, and calculate tolerance for each predictor.\n\nmap(list(model1, model2), check_collinearity)\n\nNot enough model terms in the conditional part of the model to check for\n  multicollinearity.\n\n\n[[1]]\nNULL\n\n[[2]]\n# Check for Multicollinearity\n\nLow Correlation\n\n           Term  VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n log(revmoyfoy) 3.47 [ 3.28,  3.67]     1.86      0.29     [0.27, 0.30]\n           petr 1.10 [ 1.06,  1.14]     1.05      0.91     [0.87, 0.94]\n          pagri 1.28 [ 1.23,  1.33]     1.13      0.78     [0.75, 0.81]\n            age 2.14 [ 2.03,  2.25]     1.46      0.47     [0.44, 0.49]\n  log(prixbien) 2.54 [ 2.41,  2.68]     1.59      0.39     [0.37, 0.41]\n          pindp 3.59 [ 3.39,  3.80]     1.89      0.28     [0.26, 0.29]\n\nModerate Correlation\n\n   Term  VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n  pouvr 9.36 [ 8.79,  9.96]     3.06      0.11     [0.10, 0.11]\n  pempl 8.89 [ 8.35,  9.46]     2.98      0.11     [0.11, 0.12]\n  ppint 8.92 [ 8.39,  9.50]     2.99      0.11     [0.11, 0.12]\n   psup 5.09 [ 4.80,  5.40]     2.26      0.20     [0.19, 0.21]\n pnodip 5.25 [ 4.95,  5.58]     2.29      0.19     [0.18, 0.20]\n\nHigh Correlation\n\n  Term   VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n pcadr 11.27 [10.58, 12.00]     3.36      0.09     [0.08, 0.09]\n\n\nYou see here that for instance “pcadr” (% Executives) is highly correlated with the rest of the variables, which is expected, since you can express cadre as a combination of all the other variables, - which also explains why the other variables are also very correlated. The more you add variables about the occupational structure, the more they predict the others, because \\[\\begin{equation}\n\\text{pcadr = 1 - (pouvr + pempl+ ppint+psup+pnodip)}\n\\end{equation}\\]\nwhich is why pcadr is unsignificant."
  },
  {
    "objectID": "sessions/session1/session1.html#heteroskedasticity",
    "href": "sessions/session1/session1.html#heteroskedasticity",
    "title": "Session 1 — R reminder and OLS",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\nHeteroskedasticity occurs in regression analysis when the spread of the residuals (or errors) varies depending on the values of the independent variables. This contradicts the classical linear regression assumption of homoskedasticity, which requires that the residuals have a constant variance.\nWhen heteroskedasticity is present, it can undermine the accuracy of statistical conclusions. The standard errors of the estimated coefficients may become unreliable, resulting in misleading p-values and confidence intervals. To detect heteroskedasticity, you can use diagnostic tools like plotting residuals against predicted values. Statistical tests, such as the Breusch-Pagan test, can also formally evaluate whether the residuals’ variance remains constant. If heteroskedasticity is confirmed, you may need to adjust the model, apply data transformations, or use robust standard errors to mitigate its impact.\n\naugment(model2) |&gt;  \n  ggplot(aes(.fitted, .resid)) + \n  geom_point()\n\n\n\n\n\n\n\ncheck_heteroscedasticity(model2) # Run a Breush-Pagan test\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.016)."
  },
  {
    "objectID": "sessions/session1/session1.html#multicollinearity-1",
    "href": "sessions/session1/session1.html#multicollinearity-1",
    "title": "Session 1 — R reminder and OLS",
    "section": "Multicollinearity",
    "text": "Multicollinearity"
  },
  {
    "objectID": "sessions/session1/session1.html#outliers",
    "href": "sessions/session1/session1.html#outliers",
    "title": "Session 1 — R reminder and OLS",
    "section": "Outliers",
    "text": "Outliers\nTo detect outliers, we use the cook distance which is an outlier detection methods. It estimate how much our regression coefficients change if we remove each observation.\n\ncheck_outliers(model2)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.949).\n- For variable: (Whole model)"
  },
  {
    "objectID": "sessions/session1/session1.html#assumptions",
    "href": "sessions/session1/session1.html#assumptions",
    "title": "Session 1 — R reminder and OLS",
    "section": "Assumptions",
    "text": "Assumptions\nFor this relation to hold, the following hypotheses must be true :\n\n\\(\\mathbb{E} (\\epsilon_i) = 0\\). That basically means that the expectation of the error should be (there is no systematic error in your estimation).\n\\(\\mathbb{E} (\\epsilon_i^2) = \\sigma^2\\). That means that the variance of the error is constant accross all values. This condition is called ‘homoskedasticity’. The case where \\(\\mathbb{E} (\\epsilon_i^2) \\ne \\sigma^2\\) is called heteroskedasticity.\n\\(\\mathbb{E} (\\epsilon_i \\epsilon_j)= 0\\) or \\(\\mathbb{C}ov(\\epsilon_i, \\epsilon_j) = 0\\), which is called the linear independence of the errors assumption.\n\\(X_{ij}\\) is non random \\(\\forall  i = 1,...,n ; j = 1, ..., k\\) (X is not a random variable, contrary to the error and the dependent variable).\n\\(\\text{rank}((X_{ij})) = k\\). This is the mathematical way to say that the different independent variables are indeed independent from one another. That means that there is no way to obtain any \\(X_j\\) using sums and combinations of all the other \\(X_k\\) (where \\(k \\ne j\\)). You will understand that later, in the last section of this class."
  },
  {
    "objectID": "sessions/session1/session1.html#linear-regression-in-r",
    "href": "sessions/session1/session1.html#linear-regression-in-r",
    "title": "Session 1 — R reminder and OLS",
    "section": "Linear regression in R",
    "text": "Linear regression in R\nLuckily for us, the linear estimator is present in base R, it’s called lm and we can use it straight for estimating regression coefficients : we just need to give our data-set and the relationship between our dependent and independent variable, and the machine does the rest for us ! We can also complete our initial graph with the geom_smooth function, that will draw this line that is the best line fitting the data, for us.\n\nmodel1 &lt;- lm(data = elections2024 %&gt;% filter(dens == \"Urbain intermédiaire\"), far_right_relvotes ~ log(revmoyfoy))\n\nelections2024 %&gt;% filter(dens == \"Urbain intermédiaire\") %&gt;% ggplot(aes(x = log(revmoyfoy), y = far_right_relvotes)) + geom_point()+ labs(y = \"RN vote\", x = \"Median income in commune (log scale)\")+ geom_smooth(method = \"lm\", linetype = \"dashed\", color = \"black\", se = TRUE, linewidth = 1, alpha = 0.4)+theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nsummary(model1)\n\n\nCall:\nlm(formula = far_right_relvotes ~ log(revmoyfoy), data = elections2024 %&gt;% \n    filter(dens == \"Urbain intermédiaire\"))\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.312869 -0.059556  0.000578  0.061612  0.245735 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     2.310232   0.054997   42.01   &lt;2e-16 ***\nlog(revmoyfoy) -0.185713   0.005257  -35.33   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08438 on 3490 degrees of freedom\nMultiple R-squared:  0.2634,    Adjusted R-squared:  0.2632 \nF-statistic:  1248 on 1 and 3490 DF,  p-value: &lt; 2.2e-16\n\n\nIf you want a neater presentation of the regression table, you can use the broom package :\n\nlibrary(broom)\ntidy(model1)\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic   p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)       2.31    0.0550       42.0 1.84e-312\n2 log(revmoyfoy)   -0.186   0.00526     -35.3 5.31e-234"
  }
]